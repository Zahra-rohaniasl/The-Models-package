In today’s lab, you’ll practice building workflowsets with recipes, parsnip models, rsample cross validations, model tuning and model comparison.

Learning goals
By the end of the lab you will…

Be able to build workflows to evaluate different models and featuresets.
## Setup


```{r}
library(magrittr)   # the pipe
library(tidyverse)  # for data wrangling + visualization
library(tidymodels) # for modeling
library(ggplot2)    # for plotting
# set the efault theme for plotting
theme_set(theme_bw(base_size = 18) + theme(legend.position = "top"))
```


The Data
Today we will be using the Ames Housing Data.

This is a data set from De Cock (2011) has 82 fields were recorded for 2,930 properties in Ames Iowa in the US. The version in the modeldata package is copied from the AmesHousing package but does not include a few quality columns that appear to be outcomes rather than predictors.

```{r}
dat <- modeldata::ames
```

The data dictionary can be found on the internet:

```{r}
cat(readr::read_file("http://jse.amstat.org/v19n3/decock/DataDocumentation.txt"))
```


Exercise 1: EDA
Write and execute the code to perform summary EDA on the Ames Housing data using the package skimr.

SOLUTION:
```{r}
dat %>% skimr::skim()
```

Exercise 2: Train / Test Splits
Write and execute code to create training and test datasets. Have the training dataset represent 75% of the total data.

SOLUTION:
```{r}
set.seed(8740)
# split the data, with 75% in the training set
data_split <- rsample::initial_split(dat, strata = "Sale_Price", prop = 0.75)

# extract the training set
ames_train <- rsample::training(data_split)
# extract the text set
ames_test  <- rsample::testing(data_split)
```

Exercise 3: Data Preprocessing
create a recipe based on the formula Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold with the following steps:

transform the outcome variable Sale_Price to log(Sale_Price) (natural log)
center and scale all numeric predictors
transform the categorical variable Neighborhood to pool infrequent values (see recipes::step_other)
create dummy variables for all nominal predictors
Finally prep the recipe.

Make sure you consider the order of the operations (hint: step_dummy turns factors into multiply integer (numeric) predictor, so consider when to scale numeric predictors relative to creating dummy predictors.

You can use broom::tidy() on the recipe to examine whether the prepped data is correct.

SOLUTION:
```{r}
norm_recipe <- 
  # create a recipe with the specified formula and data
  recipes::recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  # center all predictors
  recipes::step_center(all_numeric_predictors()) %>%
  # scales all predictors
  recipes::step_scale(all_numeric_predictors()) %>%
  # transformm the outcome using log-base-e (natural log)
  recipes::step_log(Sale_Price, base = exp(1)) %>% 
  # pool categories with few members into a new category - 'other'
  recipes::step_other(Neighborhood) %>% 
  # create dummy variables for all categories
  recipes::step_dummy(all_nominal_predictors()) %>%
  # estimate the means and standard deviations by prepping the data
  recipes::prep(training = ames_train, retain = TRUE)

norm_recipe %>% broom::tidy()


```


Exercise 4 Modeling
Create three regression models using the parsnip:: package and assign each model to its own variable

a base regression model using lm
a regression model using glmnet; set the model parameters penalty and mixture for tuning
a tree model using the ranger engine; set the model parameters min_n and trees for tuning
SOLUTION:
```{r}
# linear model: engine lm, mode regression
lm_mod_base <- parsnip::linear_reg() %>%
  parsnip::set_engine("lm")  %>% 
  parsnip::set_mode("regression")

# tuned linear model: engine glmnet, mode regression; tuning penalty and mixture
lm_mod_glmnet <- 
  parsnip::linear_reg( penalty = parsnip::tune(), mixture = parsnip::tune() ) %>% 
  parsnip::set_engine("glmnet") %>% 
  parsnip::set_mode("regression")

# random forest model: engine lm, mode regression
lm_mod_rforest <- 
  parsnip::rand_forest( min_n = parsnip::tune(), trees = parsnip::tune() ) %>% 
  parsnip::set_engine("ranger") %>% 
  parsnip::set_mode("regression")

lm_mod_base
lm_mod_glmnet
lm_mod_rforest

```


Exercise 5
Use parsnip::translate() on each model to see the model template for each method of fitting.

SOLUTION:
```{r}
lm_mod_base %>% parsnip::translate()
lm_mod_glmnet %>% parsnip::translate()
lm_mod_rforest %>% parsnip::translate()
```

Exercise 6 Bootstrap
Create bootstrap samples for the training dataset. You can leave the parameters set to their defaults

SOLUTION:
```{r}
set.seed(8740)
train_resamples <- rsample::bootstraps(ames_train)
```

Exercise 7
Create workflows with workflowsets::workflow_set using your recipe and models. Show the resulting datastructure, noting the number of columns, and then use tidyr:: to unnest the info column and show its contents.

SOLUTION:
```{r}
all_workflows <- 
  workflowsets::workflow_set(
    preproc = list(base = norm_recipe),
    models = list(base = lm_mod_base, glmnet = lm_mod_glmnet, forest = lm_mod_rforest)
  )

all_workflows # four columns
all_workflows %>% tidyr::unnest(info)
```


Exercise 8
Use workflowsets::workflow_map to map the default function (tune::tune_grid() - look at the help for workflowsets::workflow_map ) across the workflows in the workflowset you just created and update the variable all_workflows with the result.

```{r}
all_workflows <- all_workflows %>% 
  workflowsets::workflow_map(
    verbose = TRUE                # enable logging
    , resamples = train_resamples # a parameter passed to tune::tune_grid()
    , grid = 5                    # a parameter passed to tune::tune_grid()
  )
```

The updated variable all_workflows contains a nested column named result, and each cell of the column result is a tibble containing a nested column named .metrics. Write code to

un-nest the metrics in the column .metrics
filter out the rows for the metric rsq
group by wflow_id, order the .estimate column from highest to lowest, and pick out the first row of each group.
SOLUTION:
```{r}
all_workflows %>% 
  # unnest the result column
  dplyr::select(wflow_id,result) %>% 
  tidyr::unnest(result) %>% 
  # unnest the .metrics column
  tidyr::unnest(.metrics) %>% 
  # filter out the metric rsq
  dplyr::filter(.metric == 'rsq') %>% 
  # group by wflow_id
  dplyr::group_by(wflow_id) %>% 
  # order by .estimate, starting with the largest value
  dplyr::arrange(desc(.estimate) ) %>% 
  # select the first row for each group (i.e. highest rsq)
  dplyr::slice(1)

```


Exercise 9
Run the code below and compare to your results from exercise 8.

SOLUTION:
```{r}
workflowsets::rank_results(
  all_workflows
  , rank_metric = 'rsq'
  , select_best = TRUE
)
```

Exercise 10
Select the best model per the rsq metric using its id.

SOLUTION:
```{r}
# extract the best model (per rsq)
best_model_workflow <- 
  all_workflows %>% 
  workflowsets::extract_workflow("base_forest")

# finalize the workflow
best_model_workflow <- 
  best_model_workflow %>% 
  tune::finalize_workflow(
    tibble::tibble(trees = 1971, min_n = 2) # enter the name and value of the best-fit parameters
  ) 

# having trained the model, compare test and training performance
training_fit <- best_model_workflow %>% 
  fit(data = ames_train)
training_fit
testing_fit <- best_model_workflow %>% 
  fit(data = ames_test)
testing_fit

```


What is the ratio of the OOB prediction errors (MSE): test/train?

The ratio is 0.06821716 / 0.05747611 - 1 = 0.1868785, or about 19% higher in the test dataset than in the training dataset.
